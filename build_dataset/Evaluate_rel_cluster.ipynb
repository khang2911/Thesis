{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\anaconda1\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster = 10\n",
    "X = np.load('../data/PCA/sen_vector_PCA.npy')\n",
    "kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "pred_label = kmeans.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "ref = json.load(open('../data/preprocessing/standard_ref/standard_ref_without_effect.json'))\n",
    "\n",
    "sentences = json.load(open('../data/feature_extraction/entity_pair_sentences.json'))\n",
    "\n",
    "entity_names = open('../data/clean_data/entity_total.csv',encoding='utf8').read().split('\\n')[:-1]\n",
    "\n",
    "#####\n",
    "path = '../data/model/noun_verb/300/model'\n",
    "model = Word2Vec.load(path)\n",
    "voca = list(model.wv.vocab)\n",
    "model = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = {}\n",
    "for row in entity_names :\n",
    "    entity_dict[row.split('\\t')[1]] = row.split('\\t')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = 0\n",
    "total = 0\n",
    "ref_pair = []\n",
    "for herb in ref :\n",
    "    for drug in ref[herb] :\n",
    "        if entity_dict[herb] in voca and entity_dict[drug] in voca :\n",
    "            ref_pair.append([entity_dict[drug],entity_dict[herb]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pair = []\n",
    "for each in sentences :\n",
    "    entity_pair.append(sorted([each[1][each[0][0]],each[1][each[0][1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9: 16, 0: 4, 6: 1, 7: 1, 4: 1}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "result = {}\n",
    "for each in ref_pair :\n",
    "    if each in entity_pair :\n",
    "        group = pred_label[entity_pair.index(each)]\n",
    "        #if group == 9 :\n",
    "            #print(each)\n",
    "        try :\n",
    "            result[group]+=1\n",
    "        except :\n",
    "            result[group] = 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "##### cluster labeling - cosine similarity\n",
    "centroids = kmeans.cluster_centers_\n",
    "res = {}\n",
    "\n",
    "for i in range(len(pred_label)) :\n",
    "    cosine = 1 - spatial.distance.cosine(X[i],centroids[pred_label[i]] )\n",
    "    if pred_label[i] not in res :\n",
    "        res[pred_label[i]] = {cosine:i}\n",
    "    else :\n",
    "        res[pred_label[i]][cosine] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select', 'pressur', 'act', 'preserv', 'photosystem', 'marin', 'micro', 'drug_entity_3149', 'l', 'lineages.a', 'studi', 'conduct', 'herb_entity_7643', 'forest', 'produc', 'quantiti', 'mushroom', 'porcini', 'boletu', 'eduli', 'lato']\n",
      "\n",
      "\n",
      "['result', 'obtain', 'impli', 'explan', 'origin', 'herb_entity_2909', 'comb', 'phase', 'term', 'drug_entity_10566', 'forc', 'discount', 'perform', 'investig', 'passiv', 'effect', 'properti', 'si', 'x', 'ge', 'nanowir']\n",
      "\n",
      "\n",
      "['drug_entity_3149', 'l', 'effect', 'herb_entity_10078', 'extract', 'infus', 'microorgan', 'evalu', 'listeria', 'monocytogen', 'extract', 'sampl', 'analyz', 'control', 'show', 'effect', 'mg/ml', 'mg/l', 'concentr']\n",
      "\n",
      "\n",
      "['lesion', 'model', 'pd', 'exploit', 'develop', 'gene', 'therapi', 'transplant', 'approaches.th', 'discoveri', 'receptor', 'drug_entity_458', 'compon', 'marijuana', 'open', 'horizon', 'exploit', 'herb_entity_6418', 'cannabinoid']\n",
      "\n",
      "\n",
      "['find', 'transit', 'relat', 'select', 'sector', 'manifold', 'phase', 'obtain', 'map', 'manifold', 'dimer', 'model', 'herb_entity_2909', 'comb', 'superlattice.w', 'studi', 'tr', 'drug_entity_1296', 'process', 'particl', 'chain', 'caviti', 'interact', 'potenti']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top =[ res[9][i] for i in sorted(res[9])[:5] ]\n",
    "for i in top :\n",
    "    print(sentences[i][1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daviesbouldin(X, labels, centroids):\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.spatial.distance import pdist, euclidean\n",
    "\n",
    "    nbre_of_clusters = len(centroids) #Get the number of clusters\n",
    "    distances = [[] for e in range(nbre_of_clusters)] #Store intra-cluster distances by cluster\n",
    "    distances_means = [] #Store the mean of these distances\n",
    "    DB_indexes = [] #Store Davies_Boulin index of each pair of cluster\n",
    "    second_cluster_idx = [] #Store index of the second cluster of each pair\n",
    "    first_cluster_idx = 0 #Set index of first cluster of each pair to 0\n",
    "\n",
    "    # Step 1: Compute euclidean distances between each point of a cluster to their centroid\n",
    "    for cluster in range(nbre_of_clusters):\n",
    "        for point in range(X[labels == cluster].shape[0]):\n",
    "            distances[cluster].append(euclidean(X[labels == cluster][point], centroids[cluster]))\n",
    "\n",
    "    # Step 2: Compute the mean of these distances\n",
    "    for e in distances:\n",
    "        distances_means.append(np.mean(e))\n",
    "\n",
    "    # Step 3: Compute euclidean distances between each pair of centroid\n",
    "    ctrds_distance = pdist(centroids) \n",
    "\n",
    "    # Tricky step 4: Compute Davies-Bouldin index of each pair of cluster   \n",
    "    for i, e in enumerate(e for start in range(1, nbre_of_clusters) for e in range(start, nbre_of_clusters)):\n",
    "        second_cluster_idx.append(e)\n",
    "        if second_cluster_idx[i-1] == nbre_of_clusters - 1:\n",
    "            first_cluster_idx += 1\n",
    "        DB_indexes.append((distances_means[first_cluster_idx] + distances_means[e]) / ctrds_distance[i])\n",
    "\n",
    "    # Step 5: Compute the mean of all DB_indexes   \n",
    "    print(\"DAVIES-BOULDIN Index: %.5f\" % np.mean(DB_indexes)) \n",
    "    return np.mean(DB_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVIES-BOULDIN Index: 2.15218\n",
      "DAVIES-BOULDIN Index: 1.57891\n",
      "DAVIES-BOULDIN Index: 1.64996\n",
      "DAVIES-BOULDIN Index: 1.67187\n",
      "DAVIES-BOULDIN Index: 1.68358\n",
      "DAVIES-BOULDIN Index: 1.23870\n",
      "DAVIES-BOULDIN Index: 1.25308\n",
      "DAVIES-BOULDIN Index: 1.28893\n",
      "DAVIES-BOULDIN Index: 1.28271\n",
      "DAVIES-BOULDIN Index: 1.30754\n",
      "DAVIES-BOULDIN Index: 1.33104\n",
      "DAVIES-BOULDIN Index: 1.32987\n",
      "DAVIES-BOULDIN Index: 1.34020\n",
      "DAVIES-BOULDIN Index: 1.32118\n",
      "DAVIES-BOULDIN Index: 1.32412\n",
      "DAVIES-BOULDIN Index: 1.33698\n",
      "DAVIES-BOULDIN Index: 1.33346\n",
      "DAVIES-BOULDIN Index: 1.34397\n",
      "DAVIES-BOULDIN Index: 1.34838\n",
      "DAVIES-BOULDIN Index: 1.34047\n",
      "DAVIES-BOULDIN Index: 1.32625\n",
      "DAVIES-BOULDIN Index: 1.32545\n",
      "DAVIES-BOULDIN Index: 1.32743\n",
      "DAVIES-BOULDIN Index: 1.31306\n",
      "DAVIES-BOULDIN Index: 1.32498\n",
      "DAVIES-BOULDIN Index: 1.29528\n",
      "DAVIES-BOULDIN Index: 1.32168\n",
      "DAVIES-BOULDIN Index: 1.29037\n",
      "DAVIES-BOULDIN Index: 1.31982\n",
      "DAVIES-BOULDIN Index: 1.28982\n",
      "DAVIES-BOULDIN Index: 1.29385\n",
      "DAVIES-BOULDIN Index: 1.27049\n",
      "DAVIES-BOULDIN Index: 1.28276\n",
      "DAVIES-BOULDIN Index: 1.27956\n",
      "DAVIES-BOULDIN Index: 1.27335\n",
      "DAVIES-BOULDIN Index: 1.26467\n",
      "DAVIES-BOULDIN Index: 1.25702\n",
      "DAVIES-BOULDIN Index: 1.24613\n",
      "DAVIES-BOULDIN Index: 1.24784\n",
      "DAVIES-BOULDIN Index: 1.26976\n",
      "DAVIES-BOULDIN Index: 1.21928\n",
      "DAVIES-BOULDIN Index: 1.23196\n",
      "DAVIES-BOULDIN Index: 1.24110\n",
      "DAVIES-BOULDIN Index: 1.21391\n",
      "DAVIES-BOULDIN Index: 1.21781\n",
      "DAVIES-BOULDIN Index: 1.20975\n",
      "DAVIES-BOULDIN Index: 1.23948\n",
      "DAVIES-BOULDIN Index: 1.22006\n"
     ]
    }
   ],
   "source": [
    "davie = {}\n",
    "calinski = {}\n",
    "X = np.load('../data/PCA/sen_vector_PCA.npy')\n",
    "for n_cluster in range(2,50) :\n",
    "    kmeans = KMeans(n_clusters=n_cluster, max_iter=1000).fit(X)\n",
    "    labels = kmeans.predict(X)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    davie[n_cluster] = daviesbouldin(X, labels, centroids)\n",
    "    calinski[n_cluster] =  sklearn.metrics.calinski_harabaz_score(X,labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
